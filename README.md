# Everlast AI Backend

**Version 1.0** | Lokaler KI-Server fÃ¼r Offline-Verarbeitung

> Companion-Projekt zu [Everlast AI](https://github.com/goettemar/everlast-ai)

---

## Was ist das?

Ein standalone KI-Server fÃ¼r **lokale LLM-Generierung** (Ollama) und **Audio-Transkription** (faster-whisper). ErmÃ¶glicht vollstÃ¤ndig private Verarbeitung ohne Cloud-APIs.

```
ğŸ¤ Audio  â†’  ğŸ“ Transkription (Whisper)  â†’  ğŸ¤– KI-Verarbeitung (Ollama)
              (lokal, GPU-beschleunigt)       (lokal, privat)
```

### Highlights

- **Privacy-Mode** â€“ Alle Daten bleiben auf deinem Rechner
- **Keine API-Kosten** â€“ Einmalige Modell-Downloads, dann kostenlos
- **Automatisches Setup** â€“ start.sh installiert und konfiguriert alles
- **GPU-Optimiert** â€“ Automatische Modell-Empfehlung fÃ¼r deine Hardware

---

## Schnellstart

### Linux / macOS

```bash
# Repository klonen
git clone https://github.com/goettemar/everlast_ai_backend.git
cd everlast_ai_backend

# Server starten (interaktives Setup beim ersten Mal)
./start.sh
```

### Windows (Beta)

> **Hinweis:** Die Windows-Version wurde nicht ausfÃ¼hrlich getestet und gilt als Beta. Feedback und Bug-Reports sind willkommen!

```powershell
# Repository klonen
git clone https://github.com/goettemar/everlast_ai_backend.git
cd everlast_ai_backend

# Server starten (interaktives Setup beim ersten Mal)
start.bat
```

Das Start-Skript:
1. PrÃ¼ft Python und Ollama Installation
2. Bietet automatische Ollama-Installation an
3. LÃ¤dt empfohlene Modelle herunter (Whisper medium, DeepSeek-R1)
4. Konfiguriert Everlast AI fÃ¼r lokale Provider
5. Startet den Server auf Port 8080

**API-Dokumentation:** http://localhost:8080/docs

---

## Voraussetzungen

| Komponente | Minimum | Empfohlen |
|------------|---------|-----------|
| Python | 3.10+ | 3.11+ |
| RAM | 8 GB | 16 GB |
| GPU VRAM | - (CPU mÃ¶glich) | 8+ GB |
| Speicher | 10 GB | 20 GB |

### Ollama (fÃ¼r LLM)

**Linux/macOS:** Wird automatisch vom Start-Skript installiert, oder manuell:

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull deepseek-r1:8b
```

**Windows:** Installer von [ollama.com/download/windows](https://ollama.com/download/windows) herunterladen und installieren, dann:

```powershell
ollama pull deepseek-r1:8b
```

---

## GPU-Profile

Das System erkennt automatisch deine GPU und wÃ¤hlt passende Modelle:

| Profil | VRAM | LLM-Empfehlung | STT-Empfehlung | QualitÃ¤t |
|--------|------|----------------|----------------|----------|
| **cpu** | - | llama3.2:1b | small | â˜…â˜…â˜†â˜†â˜† |
| **8gb** | 8 GB | deepseek-r1:8b | medium | â˜…â˜…â˜…â˜…â˜† |
| **16gb** | 16 GB | llama3.2:8b | large-v3 | â˜…â˜…â˜…â˜…â˜… |
| **24gb** | 24+ GB | llama3.1:70b-q4 | large-v3 | â˜…â˜…â˜…â˜…â˜… |

---

## API-Endpunkte

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/health` | GET | Status + verfÃ¼gbare Modelle |
| `/api/v1/generate` | POST | LLM Text-Generierung |
| `/api/v1/transcribe` | POST | Audio-Transkription |
| `/api/v1/models` | GET | Liste der Ollama-Modelle |
| `/api/v1/gpu-profiles` | GET | GPU-Profile mit Empfehlungen |

### Beispiel: Text generieren

```bash
curl -X POST http://localhost:8080/api/v1/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Fasse diesen Text zusammen: ..."}'
```

### Beispiel: Audio transkribieren

```bash
curl -X POST http://localhost:8080/api/v1/transcribe \
  -F "audio=@meeting.webm" \
  -F "language=de"
```

---

## Konfiguration

Umgebungsvariablen (oder `.env`-Datei):

| Variable | Default | Beschreibung |
|----------|---------|--------------|
| `BACKEND_PORT` | `8080` | Server-Port |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama-Server URL |
| `WHISPER_MODEL` | `auto` | STT-Modell (tiny/base/small/medium/large-v3) |
| `GPU_PROFILE` | `auto` | Profil (8gb/16gb/24gb/cpu) |

### Kommandozeilen-Optionen

**Linux/macOS:**
```bash
./start.sh              # Normaler Start
./start.sh --setup      # Setup erneut ausfÃ¼hren
./start.sh --reset      # Setup zurÃ¼cksetzen
./start.sh --help       # Hilfe anzeigen
```

**Windows:**
```powershell
start.bat               # Normaler Start
start.bat --setup       # Setup erneut ausfÃ¼hren
start.bat --reset       # Setup zurÃ¼cksetzen
start.bat --help        # Hilfe anzeigen
```

---

## Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Everlast AI Backend                       â”‚
â”‚                      (FastAPI Server)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚   â”‚  OllamaService  â”‚        â”‚ WhisperService  â”‚            â”‚
â”‚   â”‚  (HTTP Client)  â”‚        â”‚(faster-whisper) â”‚            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚            â”‚                          â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                          â”‚
             â–¼                          â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Ollama    â”‚           â”‚  GPU (CUDA)  â”‚
      â”‚   Server     â”‚           â”‚  oder CPU    â”‚
      â”‚ :11434       â”‚           â”‚              â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Integration mit Everlast AI

Das Start-Skript konfiguriert Everlast AI automatisch. Manuelle Konfiguration:

1. In Everlast AI: Settings â†’ Tab "KI-Backend"
2. **Ollama aktivieren**: URL `http://localhost:11434`
3. **Local Whisper aktivieren**: URL `http://localhost:8080`
4. "Verbindung testen" klicken
5. "Speichern"

---

## Projektstruktur

```
everlast_ai_backend/
â”œâ”€â”€ main.py              # FastAPI Entry
â”œâ”€â”€ config.py            # Settings + GPU-Profile
â”œâ”€â”€ start.sh             # Start-Skript mit Setup
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ routes.py        # API-Endpunkte
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ollama_service.py    # LLM-Client
â”‚   â””â”€â”€ whisper_service.py   # STT-Engine
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py       # Pydantic Models
â”‚
â””â”€â”€ requirements.txt     # Python Dependencies
```

---

## Fehlerbehebung

| Problem | LÃ¶sung |
|---------|--------|
| Ollama nicht gefunden | `curl -fsSL https://ollama.com/install.sh \| sh` |
| Modell nicht geladen | `ollama pull deepseek-r1:8b` |
| Port belegt | `BACKEND_PORT=8081 ./start.sh` |
| GPU nicht erkannt | CUDA-Treiber prÃ¼fen, `nvidia-smi` |
| Whisper langsam | GPU-Profil auf `cpu` setzen oder CUDA installieren |

---

## Lizenz

MIT License

---

*Everlast AI Backend â€“ Lokale KI fÃ¼r [Everlast AI](https://github.com/goettemar/everlast-ai)*
